{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MATH&ML-6. Математический анализ в контексте задачи оптимизации. Часть III\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Градиентный спуск: применение и модификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 2.7\n",
    "# Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии\n",
    "# Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Загрузите стандартный датасет об алмазах из библиотеки Seaborn:\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('diamonds')\n",
    "# Удалите часть признаков:\n",
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)\n",
    "# Закодируйте категориальные признаки:\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "# Логарифмируйте признаки:\n",
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])\n",
    "# Определите целевую переменную и предикторы:\n",
    "X = df.drop(columns=\"price\")\n",
    "y = df[\"price\"]\n",
    "# Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33), значение random_state должно быть равно 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском (класс SGDRegressor)\n",
    "reg = SGDRegressor()\n",
    "# Отберите с помощью GridSearchCV оптимальные параметры по следующей сетке:\n",
    "grid = {\n",
    "    \"loss\": [\"squared_error\", \"epsilon_insensitive\"],\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"alpha\": np.logspace(-3, 3, 10),\n",
    "    \"l1_ratio\": np.linspace(0, 1, 10),\n",
    "    \"learning_rate\": [\"constant\"],\n",
    "    \"eta0\": np.logspace(-4, -1, 4)\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=reg,\n",
    "    param_grid=grid,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "grid_search.fit(X_train, y_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "round(metrics.mean_squared_error(y_test, y_test_pred), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Метод Ньютона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6286680781673306"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.6296335078534031\n",
    "f = 6*x**5 - 5*x**4 - 4*x**3 + 3*x**2\n",
    "dx = 30*x**4 - 20*x**3 - 12*x**2 + 6*x\n",
    "x - f / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.1\n",
    "def newton(x):\n",
    "    def f(x):\n",
    "        return 6*x**5 - 5*x**4 - 4*x**3 + 3*x**2\n",
    "    def dx(x):\n",
    "        return 30*x**4 - 20*x**3 - 12*x**2 + 6*x\n",
    "    while True:\n",
    "        x_new = x - f(x) / dx(x)\n",
    "        if abs(x - x_new) < 0.001:\n",
    "            print(round(x_new, 3))\n",
    "            break\n",
    "        x = x_new\n",
    "\n",
    "newton(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.695121951219512\n",
      "11.734125501243229\n",
      "7.1123493600499685\n",
      "5.365000391507974\n",
      "5.015260627016227\n",
      "5.000029000201801\n",
      "5.000000000105126\n",
      "5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "initial_value = 42\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)\n",
    "\n",
    "#21.695121951219512\n",
    "#11.734125501243229\n",
    "#7.1123493600499685\n",
    "#5.365000391507974\n",
    "#5.015260627016227\n",
    "#5.000029000201801\n",
    "#5.000000000105126\n",
    "#5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newtons_method(f, fprime, x0, tol=0.0001):\n",
    "    iter_count = 0\n",
    "    x_curr = x0\n",
    "    f_val = f(x_curr)\n",
    "    while (abs(f_val) > tol):\n",
    "        f_val = f(x_curr)\n",
    "        f_prime_val = fprime(x_curr)\n",
    "        x_curr = x_curr - (f_val)/(f_prime_val)\n",
    "        iter_count += 1\n",
    "    return x_curr\n",
    "\n",
    "newtons_method(f=func1, fprime=func2, x0=50, tol=0.0001)\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import newton\n",
    "newton(func=func1, fprime=func2, x0=50, tol=0.0001)\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.727\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.6\n",
    "def f(x):\n",
    "    return x**3 - 72*x - 220\n",
    "\n",
    "def dx(x):\n",
    "    return 3*x**2 - 72\n",
    "\n",
    "x0 = 12\n",
    "\n",
    "def newton(f, dx, x):\n",
    "    while True:\n",
    "        x_new = x - f(x) / dx(x)\n",
    "        if abs(x - x_new) < 0.001:\n",
    "            print(round(x_new, 3))\n",
    "            break\n",
    "        x = x_new\n",
    "\n",
    "newton(f, dx, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.7\n",
    "def f(x):\n",
    "    return x**2 + 9*x - 5\n",
    "\n",
    "def dx(x):\n",
    "    return 2*x + 9\n",
    "\n",
    "x0 = 2.2\n",
    "\n",
    "def newton(f, dx, x):\n",
    "    while True:\n",
    "        x_new = x - f(x) / dx(x)\n",
    "        if abs(x - x_new) < 0.01:\n",
    "            print(round(x_new, 2))\n",
    "            break\n",
    "        x = x_new\n",
    "\n",
    "newton(f, dx, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167\n"
     ]
    }
   ],
   "source": [
    "# Задание 3.9\n",
    "def dx(x):\n",
    "    return 24*x**2 - 4*x\n",
    "\n",
    "def ddx(x):\n",
    "    return 48*x -4\n",
    "\n",
    "x0 = 42\n",
    "\n",
    "def newton(f, dx, x):\n",
    "    while True:\n",
    "        x_new = x - f(x) / dx(x)\n",
    "        if abs(x - x_new) < 0.0001:\n",
    "            print(round(x_new, 3))\n",
    "            break\n",
    "        x = x_new\n",
    "\n",
    "newton(dx, ddx, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Квазиньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которую будем оптимизировать. Вместо отдельных $x$ и $y$ можно взять координаты единого вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим градиент для функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим начальную точку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    " \n",
    "#Статус оптимизации Optimization terminated successfully.\n",
    "#Количество оценок: 3\n",
    "#Решение: f([0. 0.]) = 0.00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке $(0,0)$. Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией  *L-BFGS-B*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат будет тем же самым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: -0.9999999999999218\n",
       "        x: [-4.000e+00  1.000e+00]\n",
       "      nit: 4\n",
       "      jac: [ 2.900e-07  2.724e-07]\n",
       "     nfev: 9\n",
       "     njev: 9\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 4.1\n",
    "def func(x):\n",
    "    return x[0]**2 - x[0]*x[1] + x[1]**2 + 9*x[0] - 6*x[1] + 20\n",
    "\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - x[1] + 9, -x[0] + 2*x[1] - 6])\n",
    "\n",
    "x0 = [-400, -400]\n",
    "\n",
    "result = minimize(func, x0, method='L-BFGS-B', jac=grad_func)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 42.75\n",
       "        x: [ 1.500e+00]\n",
       "      nit: 4\n",
       "      jac: [ 0.000e+00]\n",
       " hess_inv: [[ 5.000e-01]]\n",
       "     nfev: 5\n",
       "     njev: 5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 4.4\n",
    "def f(x):\n",
    "    return x**2 - 3*x + 45\n",
    "\n",
    "def dx(x):\n",
    "    return 2*x - 3\n",
    "\n",
    "x0 = 10\n",
    "\n",
    "result = minimize(f, x0, method='BFGS', jac=dx)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 42.75\n",
       "        x: [ 1.500e+00]\n",
       "      nit: 2\n",
       "      jac: [ 0.000e+00]\n",
       "     nfev: 3\n",
       "     njev: 3\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 4.5\n",
    "minimize(f, x0, method='L-BFGS-B', jac=dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS: \n",
      "   message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 10.000000030008898\n",
      "        x: [ 1.316e-02  6.653e-14]\n",
      "      nit: 34\n",
      "      jac: [ 9.120e-06  7.984e-13]\n",
      " hess_inv: [[ 2.016e+02 -4.163e-09]\n",
      "            [-4.163e-09  7.317e-02]]\n",
      "     nfev: 37\n",
      "     njev: 37 \n",
      "\n",
      "\n",
      "L-BFGS-B:  \n",
      "   message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 10.00000000827103\n",
      "        x: [-9.527e-03 -2.322e-06]\n",
      "      nit: 37\n",
      "      jac: [-3.459e-06 -2.786e-05]\n",
      "     nfev: 40\n",
      "     njev: 40\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "# Задание 4.7\n",
    "def f(dot):\n",
    "    x = dot[0]\n",
    "    y = dot[1]\n",
    "    return x**4 + 6*y**2 + 10\n",
    "\n",
    "def grad_f(dot):\n",
    "    x = dot[0]\n",
    "    y = dot[1]\n",
    "    return np.array([4*x**3, 12*y])\n",
    "\n",
    "x0 = [100, 100]\n",
    "\n",
    "result_1 = minimize(f, x0, method='BFGS', jac=grad_f)\n",
    "result_2 = minimize(f, x0, method='L-BFGS-B', jac=grad_f)\n",
    "print(f'BFGS: \\n {result_1} \\n\\n\\nL-BFGS-B:  \\n {result_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Линейное программирование"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
